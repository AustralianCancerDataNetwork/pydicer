{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnUNet Data Preparation\n",
    "\n",
    "The [nnUNet](https://github.com/MIC-DKFZ/nnUNet) is a self-configuring method for deep\n",
    "learning-based biomedical image segmentation. However it does require data to be formatted in a\n",
    "specific way on the file system. In this notebook, we demonstrate some useful functionality to\n",
    "prepare a dataset converted by PyDicer for training using nnUNet.\n",
    "\n",
    "> Note: PyDicer currently only supports nnUNet v1. Contributions adding support for nnUNet v2 are\n",
    "> welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pydicer import PyDicer\n",
    "except ImportError:\n",
    "    !pip install pydicer\n",
    "    from pydicer import PyDicer\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from pydicer.utils import fetch_converted_test_data\n",
    "\n",
    "from pydicer.dataset.nnunet import NNUNetDataset\n",
    "from pydicer.dataset.structureset import StructureSet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup nnUNet\n",
    "\n",
    "Consult the [nnUNet documentation](https://github.com/MIC-DKFZ/nnUNet/tree/master/documentation) \n",
    "for details on how to install nnUNet, setup folder paths and conduct model training. The dataset\n",
    "will be prepared in the `nnUNet_raw_data_base` directory. If you already have this set in your\n",
    "environment you can remove the following cell. For demonstration purposes, we set our \n",
    "`nnUNet_raw_data_base` to a scratch directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"nnUNet_raw_data_base\"] = \"./nnScratch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PyDicer\n",
    "\n",
    "For this example, we will use the LCTSC test data which has already been converted using PyDicer.\n",
    "We also initialise our PyDicer object.\n",
    "\n",
    "For working with nnUNet, we set the PyDicer logging verbosity to `INFO`, so that we can see the\n",
    "relevant output being generated by the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = fetch_converted_test_data(\"./testdata_lctsc\", dataset=\"LCTSC\")\n",
    "pydicer = PyDicer(working_directory)\n",
    "pydicer.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Structures\n",
    "\n",
    "PyDicer uses the [structure name mapping](https://australiancancerdatanetwork.github.io/pydicer/_examples/WorkingWithStructures.html) functionality to determine which structures to train\n",
    "the nnUNet model for. Here we add a structure name mapping for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_id = \"nnunet_lctsc\"\n",
    "mapping = {\n",
    "    \"Esophagus\": [],\n",
    "    \"Heart\": [],\n",
    "    \"Lung_L\": [\"L_Lung\", \"Lung_Left\"],\n",
    "    \"Lung_R\": [\"Lung_Right\"],\n",
    "    \"SpinalCord\": [\"SC\"],\n",
    "}\n",
    "\n",
    "pydicer.add_structure_name_mapping(\n",
    "    mapping_id=mapping_id,\n",
    "    mapping_dict=mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise `NNUNetDataset` object\n",
    "\n",
    "The `NNUNetDataset` class provides the functionality to prepare a dataset from PyDicer data. Here\n",
    "we create an object of this class for use in this example. Check out the \n",
    "[documentation](https://australiancancerdatanetwork.github.io/pydicer/nnunet.html) for more\n",
    "information on how the `NNUNetDataset` class works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnunet_task_id = 123\n",
    "nnunet_task_name = \"LCTSC_Test\"\n",
    "nnunet_task_description = \"A dummy nnUNet task for demonstration purposes\"\n",
    "\n",
    "nnunet = NNUNetDataset(\n",
    "    working_directory,\n",
    "    nnunet_task_id,\n",
    "    nnunet_task_name,\n",
    "    nnunet_task_description,\n",
    "    mapping_id=mapping_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Dataset\n",
    "\n",
    "Our `NNUNetDataset` tool expects to have exactly one image and one structure set per patient\n",
    "(multi-modal training not yet supported, contributions welcome). Let's fetch our converted\n",
    "DataFrame to confirm that this is the case.\n",
    "\n",
    "If your dataset isn't yet in such as state, you can use the \n",
    "[`dataset preparation`](https://australiancancerdatanetwork.github.io/pydicer/_examples/DatasetPreparation.html)\n",
    "module in PyDicer to prepare a subset of data. Once the dataset is prepared, pass the\n",
    "`dataset_name` argument when creating the `NNUNetDataset` object above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pydicer.read_converted_data(working_directory)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataset\n",
    "\n",
    "The `check_dataset` function confirms that we have one image and one structure set per patient in \n",
    "our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnunet.check_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "\n",
    "Here we randomly split our dataset into a training and testing set. You can specify the\n",
    "`training_cases` and `testing_cases` to use in the `split_dataset` function. If these aren't\n",
    "supplied, the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "function from `sklearn` will be used. You can pass keyword arguments to this function via the\n",
    "`split_dataset` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnunet.split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Duplicate Data\n",
    "\n",
    "Now that the dataset is split, we must ensure that none of the `training_cases` are present in the\n",
    "`testing_cases`. Even if the cases have different IDs, it is possible that through anonymisation\n",
    "the same patient is anonymised to two different IDs. The `check_duplicates_train_test` function\n",
    "will check the imaging data to ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnunet.check_duplicates_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Structure Names\n",
    "\n",
    "The nnUNet requires that all structures are present for all cases (missing structures are not\n",
    "supported). The `check_structure_names` function will output a grid indicating where structures\n",
    "might be missing (or a structure name mapping is missing).\n",
    "\n",
    "If there are any cases for which any of the structures are missing, this should be resolved (by\n",
    "adding a structure mapping or remove the case from the dataset) before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = nnunet.check_structure_names()\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Overlapping Structures\n",
    "\n",
    "nnUNet (v1) is unable to handle structures which are overlapping. If there are structures which are\n",
    "overlapping, the PyDicer tool will assign the overlapping voxels to the smaller structure (to \n",
    "assign to the larger structure, set `nnunet.assign_overlap_to_largest=False`).\n",
    "\n",
    "The `check_overlapping_structures` function will log any structures which are overlapping and will\n",
    "be affected by this rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnunet.check_overlapping_structures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare nnUNet Dataset\n",
    "\n",
    "Now that all checks are complete, we can proceed with preparing the nnUNet dataset. Take a look in\n",
    "the dataset directory after the cell finishes running to confirm that everything worked as\n",
    "expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnunet_dataset_path = nnunet.prepare_dataset()\n",
    "print(f\"Dataset prepared in: {nnunet_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare nnUNet Training Scripts\n",
    "\n",
    "Consult the [nnUNet documentation](https://github.com/MIC-DKFZ/nnUNet/tree/master/documentation)\n",
    "for information on model training. The `generate_training_scripts` may help prepare a script useful\n",
    "for training the nnUNet models for the dataset which was prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some additional commands at the top of the script (useful for activating a virtual\n",
    "# environemnt)\n",
    "script_header = [\n",
    "    '# source /path/to/venv/bin/activate',\n",
    "]\n",
    "\n",
    "script_path = nnunet.generate_training_scripts(script_header=script_header)\n",
    "print(f\"Training script ready in: {script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "814af119db7f8f2860617be3dcd1d37c560587d11c65bd58c45b1679d3ee6ea4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('pydicer': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
